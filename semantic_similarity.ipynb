{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84de4105",
   "metadata": {},
   "source": [
    "<br>Dora Li\n",
    "<br>CS315 \n",
    "<br>Mar 12, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bdf73",
   "metadata": {},
   "source": [
    "## Results of applying the cosine similarity between post transcriptions and NYT articles that gathered from the NYT API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3983e8a",
   "metadata": {},
   "source": [
    "#### Detailed Steps:\n",
    "1. Write a Python function that takes a date, for example, \"2024-02-12\", and returns the list of articles for that day (extracting it from the month’s archive).\n",
    "2. Write some code that explores whether the fields \"abstract\" and \"snippet\" are always the same or often differ. Which one has more information?\n",
    "3. Write a function that given one article (in its nested structure), creates a flat dictionary with keys that are relevant for analysis: either the abstract or snippet (see point 2); lead paragraph; headline; keywords concatenated via semicolon; pub_date; document_type; section_name; and type_of_material\n",
    "4. Write another function that calls the function from point 3 on every article, to create a list of article dictionaries, and convert this list into a dataframe and then store it as a CSV file with the date-month in the title (this is important for point 5 below).\n",
    "5. Once you have done all of these in the notebook, create a Python script that can be called with a date (from a TikTok video). First, the script looks whether a CSV with cleaned articles is in our folder. If not, calls first the API function to get the articles and then the function that converts them into a CSV. Then, it loads the CSV into a dataframe and it uses filtering to get the articles for the desired date. These articles will be used for the Semantic Similarity portion of the TikTok Project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03adcae",
   "metadata": {},
   "source": [
    "#### Import Related Libraries + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b98838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61769c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90e358",
   "metadata": {},
   "source": [
    "#### Function Definitions:\n",
    "1. Generate CSV from NYT API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfa3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(date):\n",
    "    \"\"\"\n",
    "    function that takes a date, for example, \"2024-02-12\", and returns the list of articles for \n",
    "    that month (extracting it from the month’s archive)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # extract year, month, day from the date\n",
    "    datestring = date\n",
    "    dt = datetime.strptime(datestring, '%Y-%m')\n",
    "    \n",
    "    # constant \n",
    "    myAPIKey = \"iDALMAL9VFMiwzWionTqK3Ve4tFDUDAQ\"\n",
    "    \n",
    "    # access NYT API\n",
    "    URL = f\"https://api.nytimes.com/svc/archive/v1/{dt.year}/{dt.month}.json?api-key={myAPIKey}\"\n",
    "    data = requests.get(URL)\n",
    "    articles = data.json()\n",
    "    \n",
    "    # add to the list if the article index if it is of the day\n",
    "    n = len(articles['response']['docs'])\n",
    "    for i in range(n):\n",
    "            result.append(articles['response']['docs'][i])\n",
    "            \n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article):\n",
    "    \"\"\"\n",
    "    Write a function that given one article (in its nested structure), creates a flat dictionary \n",
    "    with keys that are relevant for analysis: either the abstract or snippet (see point 2); lead \n",
    "    paragraph; headline; keywords concatenated via semicolon; pub_date; document_type; section_name; \n",
    "    and type_of_material\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    # either the abstract or snippet (see point 2)\n",
    "    if len(article['abstract']) >= len(article['snippet']):\n",
    "        result['abstract/snippet']= article['abstract']\n",
    "    else:\n",
    "        result['abstract/snippet']= article['snippet']\n",
    "    result['lead_paragraph'] = article['lead_paragraph']\n",
    "    result['headline'] = article['headline']['main']\n",
    "    \n",
    "    # keywords concatenated via semicolon\n",
    "    k = \"\"\n",
    "    for keyword in article['keywords']:\n",
    "        k+=\";\" + keyword['value']\n",
    "    result['keywords'] = k[1:] #remove the first semicolon\n",
    "    \n",
    "    # others\n",
    "    result['pub_date'] = article['pub_date'][:10]\n",
    "    result['document_type'] = article['document_type']\n",
    "    result['section_name'] = article['section_name']\n",
    "    result['type_of_material'] = article['type_of_material']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_df(date):\n",
    "    \"\"\"\n",
    "    Write another function that calls the function from point 3 on every article, to create a list\n",
    "    of article dictionaries, and convert this list into a dataframe and then store it as a CSV file \n",
    "    with the date-month in the title (this is important for point 5 below).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # get all NYT article of that date\n",
    "    articles = get_articles(date)\n",
    "    # iterate through the articles and concatenate them\n",
    "    for article in articles:\n",
    "        article_dict = get_article_info(article)\n",
    "        d = pd.DataFrame([article_dict])\n",
    "        df = pd.concat([df,d])\n",
    "    df.to_csv(f\"{date}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea4b9a",
   "metadata": {},
   "source": [
    "2. Cosine Similarity for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    V1 = np.array(vec1)\n",
    "    V2 = np.array(vec2)\n",
    "    cosine = np.dot(V1, V2)/(norm(V1)*norm(V2))# edited dot product to be v1 and transpose of v2 instead of v2\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9338c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity(date, meta_data_df):\n",
    "    \"\"\"\n",
    "    Once you have done all of these in the notebook, create a Python script that can be called with \n",
    "    a date (from a TikTok video). First, the script looks whether a CSV with cleaned articles is in \n",
    "    our folder. If not, calls first the API function to get the articles and then the function that \n",
    "    converts them into a CSV. Then, it loads the CSV into a dataframe and it uses filtering to get \n",
    "    the articles for the desired date. These articles will be used for the Semantic Similarity portion \n",
    "    of the TikTok Project.\n",
    "    \n",
    "    inputs:\n",
    "    1. date is the date we are looking at\n",
    "    2. meta_data_df is a dataframe subset of all the meta data available on a specific day specified by date\n",
    "    \"\"\"\n",
    "    # check if a CSV w/ cleaned articles is in folder\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(\".\", topdown=False):\n",
    "        for name in files:\n",
    "            paths.append(name)\n",
    "    date_month = date[:7] # get the month specific csv\n",
    "    if f\"{date_month}.csv\" not in paths:\n",
    "        df_nyt = get_articles_df(date_month)\n",
    "        # print(\"check done\")\n",
    "    else:\n",
    "        # get NYT and TikTok Meta Data\n",
    "        df_nyt = pd.read_csv(f\"{date_month}.csv\")\n",
    "        #df_tiktok = meta_data_date\n",
    "        # print(\"read done\")\n",
    "\n",
    "    # get NYT and TikTok Meta Data ready\n",
    "    df_nyt = df_nyt[df_nyt[\"pub_date\"]== date]\n",
    "    df_nyt = df_nyt.fillna(\"\") # replace nan in the dataframe with empty string\n",
    "    df_tiktok = meta_data_df\n",
    "    df_tiktok = df_tiktok.fillna(\"\")\n",
    "    # print(\"data done\")\n",
    "\n",
    "    # function for generating embeddings\n",
    "    # embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "    # generate embeddings for the NYT content\n",
    "    abstract_list = embed(df_nyt['abstract/snippet'].tolist())\n",
    "    lead_paragraph_list = embed(df_nyt['lead_paragraph'].tolist())\n",
    "    headline_list = embed(df_nyt['headline'].tolist())\n",
    "    headline_val_list = df_nyt['headline'].tolist()\n",
    "    # keywords_list = embed(df_nyt['keywords'].tolist())\n",
    "    # print(\"embedding done\")\n",
    "    \n",
    "    def find_max_cosine(row):\n",
    "        \"\"\"\n",
    "        helper function to find maximum cosine similarity index and which article \n",
    "        results in the index \n",
    "        \"\"\"\n",
    "        tiktok = embed([row['suggested_words']])\n",
    "        # tiktok = embed([row['video_description']])\n",
    "        max_cosine = 0\n",
    "        article_index = 0\n",
    "        num = len(abstract_list) # number of NYT articles to compare to\n",
    "        # iterate through the article\n",
    "        for i in range(num):\n",
    "            # find the maximum cosineSimilarity between abstract, lead paragraph, headline, and keywords\n",
    "            curr_max_cosine = max(cosineSimilarity(tiktok, abstract_list[i]),\n",
    "                                  cosineSimilarity(tiktok, lead_paragraph_list[i]),\n",
    "                                  cosineSimilarity(tiktok, headline_list[i]))\n",
    "            if curr_max_cosine > max_cosine:\n",
    "                max_cosine = curr_max_cosine\n",
    "                article_index = i\n",
    "        headline = headline_val_list[article_index]\n",
    "        return (max_cosine, headline)\n",
    "    \n",
    "    df_tiktok['result'] = df_tiktok.apply(find_max_cosine, axis=1)\n",
    "    \n",
    "    return df_tiktok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(row):\n",
    "    return row['video_timestamp'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d7b91",
   "metadata": {},
   "source": [
    "try to generate cosine similarity for the entirely for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d095d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"results_cleaned.csv\")\n",
    "meta_data[\"format_date\"] = meta_data.apply(get_date, axis=1)\n",
    "dates = meta_data[\"format_date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "for date in dates:\n",
    "    df_tiktok = meta_data[meta_data['format_date']==date]  \n",
    "    df = generate_similarity(date, df_tiktok)\n",
    "    result = pd.concat([result, df])\n",
    "    print(date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00384931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS315 Project2\n\n",
   "language": "python",
   "name": ".project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
